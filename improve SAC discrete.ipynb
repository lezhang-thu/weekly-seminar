{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9c00179",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# policy improve in SAC\n",
    "\n",
    "* spinningup sac doc\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\begin{split}\n",
    "      \\mathrm{maximize}\\quad V^\\pi(s)&=\\mathop{\\mathbb{E}}_{a\\sim\\pi}[Q^\\pi(s,a)]+\\alpha H(\\pi(\\cdot|s))\\\\\n",
    "                                     &=\\mathop{\\mathbb{E}}_{a\\sim\\pi}[Q^\\pi(s,a)-\\alpha\\log\\pi(a|s)]\n",
    "  \\end{split}\n",
    "  \\end{equation*}\n",
    "* think: $\\pi$ is $\\pi_\\mathrm{new}$?\n",
    "\n",
    "  Directly optimizing $V^\\pi(s)$!!!\n",
    "  \n",
    "  注：这样做也不太合适，因为$s$的分布不知道，应该是$\\rho_\\theta(s)$，但实际数据为$\\rho_{\\theta_\\mathrm{old}}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de1fec4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- remove $\\mathop{\\mathbb{E}}_{a\\sim\\pi}$\n",
    "\n",
    "    - write it as\n",
    "    \n",
    "      \\begin{equation*}\n",
    "      \\sum_{a\\in \\mathcal{A}}\\pi(a|s)\n",
    "      \\end{equation*}\n",
    "    \n",
    "    - **DO NOT** write\n",
    "    \n",
    "      \\begin{equation*}\n",
    "      \\sum_{a\\sim \\pi(\\cdot|s)}\\pi(a|s)\n",
    "      \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb01625e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- What if $\\mathop{\\mathbb{E}}_{a\\sim\\pi'}$\n",
    "- Rewrite the expectation\n",
    "  \n",
    "  \\begin{equation*}\n",
    "  \\begin{split}\n",
    "  \\mathop{\\mathbb{E}}_{a\\sim\\pi}&=\\sum_{a\\in \\mathcal{A}}\\pi(a|s)\\\\\n",
    "                                &=\\sum_{a\\in \\mathcal{A}}\\pi'(a|s)\\frac{\\pi(a|s)}{\\pi'(a|s)}\\\\\n",
    "                                &=\\mathop{\\mathbb{E}}_{a\\sim\\pi'}\\Bigl[\\frac{\\pi(a|s)}{\\pi'(a|s)}\\Bigr]\n",
    "  \\end{split}\n",
    "  \\end{equation*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0963b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 观察结论\n",
    "\n",
    "- *出现好的trajectory（高return）之后，为什么后续的表现会下滑*\n",
    "\n",
    "  这是因为train时，trajectory的产生原本就是sampling的过程，除非策略的entropy已经很小了，否则多样性是正常且是需要的\n",
    "- *im的学习，需学习的trajectories越多越好吗*\n",
    "\n",
    "  im的学习不能对rl的学习造成过度的影响；从这个角度，若需im的trjaectories很少，那么神经网络就很容易满足im的目标，从而im这块不会带来梯度，从而不干扰rl"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
