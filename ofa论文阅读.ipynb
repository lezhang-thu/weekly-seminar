{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68bb3494",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training & Inference\n",
    "\n",
    "* cross-entropy loss\n",
    "\n",
    "  an input $x$, an instruction $s$ and an output $y$\n",
    "\n",
    "  \\begin{equation*}\n",
    "  \\mathcal{L}=-\\sum_{i=1}^{|y|}\\log P_\\theta(y_i|y_{<i},x,s)\n",
    "  \\end{equation*}\n",
    "\n",
    "* Trie-based Search\n",
    "\n",
    "  （仅仅涉及到downstream classification tasks，所以可忽略？）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dbfedb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Architecture\n",
    "\n",
    "* Transformer\n",
    "* encoder-decoder network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9af4d42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tasks & Modalities\n",
    "\n",
    "* Tasks including **pretraining** tasks, **downstream** tasks of **cross**-modal and **uni**-modal understanding and generation are all formed as **Seq2Seq** generation tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d9007c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## I/O\n",
    "\n",
    "* To enable multimodal pretraining, it is necessary to preprocess the data so that both visual and linguistic information can be jointly processed by the Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f485256",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## I/O\n",
    "\n",
    "* Object descriptions (e.g., bounding boxes and class labels) are expressed as sequences of discrete token\n",
    "    * a sequence of five discrete tokens, i.e. $[y_\\mathrm{min},x_\\mathrm{min},y_\\mathrm{max},x_\\mathrm{max},c]$\n",
    "    * each of the continuous corner coordinates is uniformly discretized into an integer between $[1,n_\\mathrm{bin}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf4bf0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* the vocabulary size is equal to number of bins + number of classes\n",
    "* This quantization scheme for the bounding boxes allows us to use a small vocabulary while achieving high precision\n",
    "* For example, a $600\\times 600$ image requires only 600 bins to achieve zero quantization error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02541961",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**object detection**\n",
    "  \n",
    "a language modeling task conditioned on the observed pixel inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c005da2e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## I/O\n",
    "\n",
    "**target-side image representations**\n",
    "\n",
    "* image quantization\n",
    "* Neural Discrete Representation Learning (paper)\n",
    "\n",
    "  3.1 Discrete Latent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c08e62",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## I/O\n",
    "\n",
    "**target-side image representations**\n",
    "\n",
    "* Taming Transformers for High-Resolution Image Synthesis (paper)\n",
    "  \n",
    "  3.1. Learning an Effective Codebook of Image Constituents for Use in Transformers"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
